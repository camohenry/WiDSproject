{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camohenry/WiDSproject/blob/main/WIDS_2024_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "# Data Preprocessing and Transformation for Machine Learning\n",
        "\n",
        "## Learning Objectives:\n",
        "\n",
        "Key concepts and techniques in data preprocessing.\n",
        "\n",
        "*   Importance of data preprocessing in machine learning.\n",
        "*   Define and understand data cleaning, data integration, data transformation, and feature selection.\n",
        "*   Implement data preprocessing in machine learning.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44OdC-OglN9D"
      },
      "source": [
        "## WiDS Datathon Data set\n",
        "A rich, real-world dataset which contains information about demographics, diagnosis and treatment options, and insurance provided about patients who were diagnosed with breast cancer. The dataset originated from Health Verity, one of the largest healthcare data ecosystems in the US. It was enriched with third party geo-demographic data to provide views into the socio economic aspects that may contribute to health equity. For this challenge, the dataset was then further enriched with zip code level climate data.\n",
        "\n",
        "Challenge task: You will be asked to predict the duration of time it takes for patients to receive metastatic cancer diagnosis.\n",
        "\n",
        "Why is this important? Metastatic TNBC is considered the most aggressive TNBC and requires most urgent and timely treatment. Unnecessary delays in diagnosis and subsequent treatment can have devastating effects in these difficult cancers. Differences in the wait time to get treatment is a good proxy for disparities in healthcare access.\n",
        "\n",
        "The primary goal of building these models is to detect relationships between demographics of the patient with the likelihood of getting timely treatment. The secondary goal is to see if climate patterns impact proper diagnosis and treatment.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iuw6-JOGf7I"
      },
      "source": [
        "## Step 1: Import the Libraries\n",
        "The foremost step of data preprocessing in machine learning includes importing some libraries. A library is basically a set of functions that can be called and used in the algorithm. There are many libraries available in different programming languages."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E23HABKHcIUr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfuBxZUGcHpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NPgBxea6-nf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Step 2: Import Data and Perform Intitial Analysis\n",
        "\n",
        "The next important step is to load the data which has to be used in the machine learning algorithm. This is the most important machine learning preprocessing step. Collected data is to be imported for further assessment.\n",
        "\n",
        "Once the data is loaded, checking for noisy or missing content is important.\n",
        "\n",
        "The following code cell loads the separate .csv files and creates the following two pandas DataFrames:\n",
        "\n",
        "* `train_df`, which contains the training set\n",
        "* `test_df`, which contains the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n9_cTveKmse"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow import feature_column\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wf7l5uNFFePS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJcM_8OWLmgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "metadata": {
        "id": "dHlOWTL2lw76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_df = pd.read_csv(\"/content/gtrain_df = pd.read_csvdrive/MyDrive/Data/test.csv\")\n",
        "train_df = pd.read_csv(\"/content/gdrive/MyDrive/Data/train.csv\")\n",
        "train_df.reindex(np.random.permutation(train_df.index)) # shuffle the training set\n",
        "display(train_df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the size of the data set\n",
        "print (\"Numbers of rows and columns in training set: \", train_df.shape)\n",
        "print (\"Number of rows and columns in testing set:\", test_df.shape)"
      ],
      "metadata": {
        "id": "YyKNQNUD0nMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wuv7J9-NZHfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print out column name and type of the training set\n",
        "print (train_df.info())"
      ],
      "metadata": {
        "id": "tK4-c6f-2DI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out different data types\n",
        "# Categorical columns\n",
        "cat_col = [col for col in train_df.columns if train_df[col].dtype == 'object']\n",
        "print('Categorical columns :',cat_col)\n",
        "# Numerical columns\n",
        "num_col = [col for col in train_df.columns if train_df[col].dtype != 'object']\n",
        "print('Numerical columns :',num_col)"
      ],
      "metadata": {
        "id": "ffZglS6PD8Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the columns that have the most missing values\n",
        "def plot_nas(df: pd.DataFrame):\n",
        "    if df.isnull().sum().sum() != 0:\n",
        "        na_df = (df.isnull().sum() / len(df)) * 100\n",
        "        na_df = na_df.drop(na_df[na_df == 0].index).sort_values(ascending=False)\n",
        "        missing_data = pd.DataFrame({'Missing Ratio %' :na_df})\n",
        "        missing_data_more_than_20_percent = missing_data[missing_data['Missing Ratio %'] > 10.0]\n",
        "        missing_data_more_than_20_percent.plot(kind = \"barh\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print('No NAs found')\n",
        "\n",
        "plot_nas(train_df)"
      ],
      "metadata": {
        "id": "slfh4KH1Y2UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot categorical columns\n",
        "\n",
        "Categoricals is the data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (categories). Examples are gender, social class, blood type, country affiliation. Plotting existing categories and number of rows per category will inform us any balance issue that data set might have.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DT81KsawrJZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['payer_type'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "gGymslnurI2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Print statistical summary of numerical columns\n",
        "\n",
        "Different from categorical columns, numerical columns contain numerical values which might be difficult to plot. With numberical columns, we can print out statistical summary of the columns for initial analysis."
      ],
      "metadata": {
        "id": "yPIT_7T1ryLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['bmi'].describe()"
      ],
      "metadata": {
        "id": "zar0jqY8bSpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the correlation between different columns (features)"
      ],
      "metadata": {
        "id": "qd9n5UJ4tRtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot correlation between categorical values vs. numerical values\n",
        "train_df.groupby(['patient_race'])[\"bmi\"].mean().plot(kind='bar')\n"
      ],
      "metadata": {
        "id": "99ReSHG8tX5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot correlation between categorical values vs. categorical values\n",
        "pd.crosstab(train_df['patient_race'],train_df['payer_type']).plot(kind=\"bar\",stacked=True)"
      ],
      "metadata": {
        "id": "-oYgTBb-yxua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Fundamental data cleaning steps\n",
        "\n",
        "While the techniques used for data cleaning may vary according to the types of data, there are several fundamental data cleaning steps that we always perform such as missing values, remove outliners.\n",
        "\n",
        "Data cleaning is often a tedious process, but it is absolutely essential to get top results and powerful insights from your data.\n"
      ],
      "metadata": {
        "id": "Kv_1f9dCzlIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Handle missing values\n",
        "\n",
        "If missing values have been found, there are particularly two ways to resolve this issue:\n",
        "\n",
        "*   Either remove the entire row that contains a missing value. However, removing the entire row can generate a possibility of losing some important data. This approach is useful if the dataset is very large.\n",
        "*   Estimate the value by taking the mean, median or mode."
      ],
      "metadata": {
        "id": "xH1ZEAqv9zAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in empty value in the \"patient_race\" column with \"N/A\"\n",
        "train_df[\"patient_race\"].fillna(\"N/A\", inplace=True)\n",
        "\n",
        "# fill in empty value in the \"patient_race\" column with \"N/A\"\n",
        "train_df[\"payer_type\"].fillna(\"N/A\", inplace=True)\n",
        "\n",
        "plot_nas(train_df)\n"
      ],
      "metadata": {
        "id": "bunDLCDZ1ynU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter out data outliers\n",
        "\n",
        "Outliers are data points that fall far outside of the norm and may skew your analysis too far in a certain direction. For example, if the average BMI value in our data set is  29.0. And normal BMI Categories are: Underweight = <18.5;\n",
        "Normal weight = 18.5–24.9; Overweight = 25–29.9; Obesity = BMI of 30 or greater. But there are values are very high (max 85 as shown in our data nalaysis). In this case, you should consider deleting this data point. This may give results that are “actually” much closer to the average."
      ],
      "metadata": {
        "id": "y-UFqlMq-UPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the BMI value to detect outliners\n",
        "sns.boxplot(train_df['bmi'])\n",
        "\n",
        "# outliner_train_df = train_df[train_df['bmi'] >70]\n",
        "# display(outliner_train_df)\n"
      ],
      "metadata": {
        "id": "YETupf8f_w6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removal_box_plot(df, column, threshold):\n",
        "    removed_outliers = df[df[column] <= threshold]\n",
        "\n",
        "    sns.boxplot(removed_outliers[column])\n",
        "    plt.title(f'Box Plot without Outliers of {column}')\n",
        "    plt.show()\n",
        "    return removed_outliers\n",
        "\n",
        "threshold_value = 70\n",
        "no_outliers = removal_box_plot(train_df, 'bmi', threshold_value)"
      ],
      "metadata": {
        "id": "K_Bylp17CF19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Data Transformation Techniques\n",
        "\n",
        "Machine learning modules cannot understand non-numeric data. It is important to transform the data in a numerical form in order to prevent any problems at later stages."
      ],
      "metadata": {
        "id": "8wQtnPxI3jf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical columns\n",
        "\n",
        "In this dataset, Payer Type is represented as a string (e.g. 'COMMERCIAL', or 'MEDICAID', 'MEDICAL ADVANTAGE'). We cannot feed strings directly to a model. Instead, we must first map them to numeric values. The categorical vocabulary columns provide a way to represent strings into numerical representation using either Label Encoding or One-hot Encoding techniques:\n",
        "\n",
        "*   Label Encoding, which consists in converting the unique values of the categorical variable into integers that follow an order. For example, the 'COMMERCIAL', or 'MEDICAID', 'MEDICAL ADVANTAGE' of payer type will be encoded respectively as 0,1 and 2.\n",
        "\n",
        "*   One-hot Encoding:  label encoding where we will assign a numerical value to these labels work. But this can add bias in our model as it will start giving higher preference to the MEDICAL ADVANTAGE parameter as 2>0 but ideally, both labels are equally important in the dataset. One-hot encoding techniques address this potenial bias issue by rather than labeling things as a number starting from 1 and then increasing for each category, we will go for more of a binary style of categorizing.\n"
      ],
      "metadata": {
        "id": "VemP9U4FFvsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Label Encoding\n",
        "le1 = preprocessing.LabelEncoder()\n",
        "train_df['payer_type_label_encode'] =le1.fit_transform(train_df['payer_type'])\n",
        "display(train_df)\n"
      ],
      "metadata": {
        "id": "3dWGHhyJ430R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One-hot Encoding\n",
        "#Get the categorical values\n",
        "one_hot_encoder = LabelBinarizer()\n",
        "one_hot_encoder.fit(train_df['payer_type'])\n",
        "print(one_hot_encoder.classes_)\n",
        "\n",
        "#Transform our payer_type column to 4 different binary columns corresponding to different categories\n",
        "transformed = pd.DataFrame(one_hot_encoder.transform(train_df['payer_type']),columns=one_hot_encoder.classes_)\n",
        "#Combine with original data frame\n",
        "train_df = pd.concat([train_df,transformed], axis = 1)\n",
        "display(train_df)"
      ],
      "metadata": {
        "id": "xkYiOZbYRrS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bucketized columns\n",
        "Often, you don't want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. Consider raw data that represents a person's age. Instead of representing age as a numeric column, we could split the age into several buckets. Notice the one-hot values below describe which age range each row matches."
      ],
      "metadata": {
        "id": "rc3t5jrq7E31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bins= [0,2,4,13,20,70,100]\n",
        "labels = ['Infant','Toddler','Kid','Teen','Adult',\"Old Adult\"]\n",
        "train_df['patient_age_group'] = pd.cut(train_df['patient_age'], bins=bins, labels=labels, right=False)\n",
        "display (train_df)"
      ],
      "metadata": {
        "id": "W_o7h-p76uSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling\n",
        "\n",
        "Scaling is the process of preprocessing the data in data analysis and ensuring that all the features in a dataset have similar ranges, making them more comparable and reducing the impact of different scales on machine learning algorithms. We can scale Pandas dataframe columns using methods like Min-max scaling, standardization, Robust scaling, and log transformation. In this article we will dive into the process of scaling pandas dataframe scaling using various methods."
      ],
      "metadata": {
        "id": "lDqBS_epGjLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_df[\"home_value\"])"
      ],
      "metadata": {
        "id": "K6xMcKxPWigo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max Scaling\n",
        "\n",
        "Min-Max scaling is also known as normalization. Using min-max scaling we can resize the data to a fixed range, typically between 0 and 1. The original distribution shape is preserved maintaining both the minimum and maximum values."
      ],
      "metadata": {
        "id": "pD3G8_BmJPfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_scaling(df, column_name):\n",
        "    min_value = df[column_name].min()\n",
        "    max_value = df[column_name].max()\n",
        "    df[column_name] = (df[column_name] - min_value) / (max_value - min_value)\n",
        "\n",
        "# Apply min-max scaling to 'Salary' column\n",
        "min_max_scaling(train_df, 'home_value')\n",
        "\n",
        "# Print the DataFrame after min-max scaling\n",
        "print(\"DataFrame after Min-Max Scaling:\")\n",
        "display(train_df[\"home_value\"])"
      ],
      "metadata": {
        "id": "yTSmgNS3Gp1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def"
      ],
      "metadata": {
        "id": "JoFg2j-_FjTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# current version of pandas generates a bunch of warnings that we'll ignore\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "import sweetviz as sv\n",
        "\n",
        "report = sv.analyze(train_df)\n",
        "report.show_html('train.html')"
      ],
      "metadata": {
        "id": "8U4hKTffFk8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zB2ZtJgfPZdF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}